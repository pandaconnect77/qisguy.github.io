<!DOCTYPE html>
<html>
<body>

<p>Here is a quote from WWF's website:</p>

<blockquote >

1	In reinforcement learning, what does the term "policy" refer to?	<br><b>1)</b>	A set of rules for feature selection	<b>2)</b>	A mapping from states to actions	<b>3)</b>	A method for updating model parameters	<b>4)</b>	A measure of prediction accuracy	<b>ANS)</b>	B	<br><br>
2	Which algorithm is specifically designed for reinforcement learning with continuous action spaces?	<br><b>1)</b>	Q-learning	<b>2)</b>	Deep Q-Networks (DQN)	<b>3)</b>	Policy Gradient methods	<b>4)</b>	Temporal Difference (T learning	<b>ANS)</b>	C	<br><br>
3	What is the purpose of exploration in reinforcement learning?	<br><b>1)</b>	To exploit the learned policy	<b>2)</b>	To maximize immediate rewards	<b>3)</b>	To minimize prediction errors	<b>4)</b>	To discover new strategies and improve learning	<b>ANS)</b>	D	<br><br>
4	What is the term used to describe a numerical representation of the expected future reward at a given state in reinforcement learning?	<br><b>1)</b>	Value function	<b>2)</b>	Policy function	<b>3)</b>	Advantage function	<b>4)</b>	Reward function	<b>ANS)</b>	A	<br><br>
5	Which reinforcement learning algorithm is known for its ability to handle environments with delayed rewards and long-term dependencies?	<br><b>1)</b>	Q-learning	<b>2)</b>	SARSA (State-Action-Reward-State-Action)	<b>3)</b>	Actor-Critic methods	<b>4)</b>	Temporal Difference (T learning	<b>ANS)</b>	C	<br><br>
6	Which technique is used to update the policy parameters in policy gradient methods?	<br><b>1)</b>	 Gradient descent	<b>2)</b>	 Random search	<b>3)</b>	 Evolutionary algorithms	<b>4)</b>	 Tabular methods	<b>ANS)</b>	A	<br><br>
7	What is the main advantage of using natural policy gradients in reinforcement learning?	<br><b>1)</b>	 They improve the stability and convergence of policy updates.	<b>2)</b>	 They provide a more efficient way to explore the state-action space.	<b>3)</b>	 They enable the agent to handle continuous action spaces.	<b>4)</b>	 They reduce the computational complexity of value function approximation.	<b>ANS)</b>	A	<br><br>
8	Which reinforcement learning algorithm is based on the idea of learning a policy directly from state observations?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Policy Gradient methods	<b>3)</b>	 Deep Q-Networks (DQN)	<b>4)</b>	 Temporal Difference (T learning	<b>ANS)</b>	B	<br><br>
9	What is the main advantage of using eligibility traces in temporal difference (T learning?	<br><b>1)</b>	 They speed up convergence by incorporating past experiences.	<b>2)</b>	 They prevent the agent from getting stuck in local optima.	<b>3)</b>	 They provide a systematic way to explore the state-action space.	<b>4)</b>	 They enable the agent to handle continuous action spaces.	<b>ANS)</b>	A	<br><br>
10	Which technique is commonly used to address the exploration-exploitation dilemma in reinforcement learning?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Value iteration	<b>3)</b>	 Q-learning	<b>4)</b>	 Epsilon-greedy strategy	<b>ANS)</b>	D	<br><br>
11	What is the main advantage of using off-policy learning in reinforcement learning?	<br><b>1)</b>	 It guarantees convergence to the optimal policy.	<b>2)</b>	 It allows the agent to learn from historical data.	<b>3)</b>	 It simplifies the computation of Q-values.	<b>4)</b>	 It enables the agent to update its policy online.	<b>ANS)</b>	B	<br><br>
12	Which technique is commonly used to estimate the value function in policy gradient methods?	<br><b>1)</b>	 Monte Carlo simulation	<b>2)</b>	 Temporal Difference (T learning	<b>3)</b>	 Bellman equation	<b>4)</b>	 Importance sampling	<b>ANS)</b>	B	<br><br>
13	What is the primary role of the critic in Actor-Critic methods?	<br><b>1)</b>	 To learn the policy by maximizing the expected reward	<b>2)</b>	 To evaluate the value of state-action pairs	<b>3)</b>	 To explore the environment and gather experience	<b>4)</b>	 To approximate the gradient of the policy function	<b>ANS)</b>	B	<br><br>
14	Which technique is commonly used to reduce the variance of policy gradient estimates?	<br><b>1)</b>	 Baseline subtraction	<b>2)</b>	 Importance sampling	<b>3)</b>	 Temporal Difference (T learning	<b>4)</b>	 Experience replay	<b>ANS)</b>	A	<br><br>
15	What is the objective of off-policy learning in reinforcement learning?	<br><b>1)</b>	 To learn the optimal policy directly from expert demonstrations	<b>2)</b>	 To update the policy based on the agent's own experiences	<b>3)</b>	 To estimate the value function using samples generated by a different policy	<b>4)</b>	 To maximize the expected reward without considering future states	<b>ANS)</b>	C	<br><br>
16	Which of the following is NOT a type of exploration strategy in reinforcement learning?	<br><b>1)</b>	 Epsilon-greedy	<b>2)</b>	 Boltzmann exploration	<b>3)</b>	 Softmax action selection	<b>4)</b>	 Value iteration	<b>ANS)</b>	D	<br><br>
17	What is the main advantage of using eligibility traces in reinforcement learning?	<br><b>1)</b>	 They speed up convergence by incorporating past experiences.	<b>2)</b>	 They prevent the agent from getting stuck in local optima.	<b>3)</b>	 They enable the agent to handle continuous action spaces.	<b>4)</b>	 They provide a systematic way to explore the state-action space.	<b>ANS)</b>	A	<br><br>
18	Which reinforcement learning algorithm is specifically designed for environments with continuous state and action spaces?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Deep Q-Networks (DQN)	<b>3)</b>	 Deterministic Policy Gradient (DPG)	<b>4)</b>	 Monte Carlo methods	<b>ANS)</b>	C	<br><br>
19	What is the main advantage of using off-policy learning in reinforcement learning?	<br><b>1)</b>	 It guarantees convergence to the optimal policy.	<b>2)</b>	 It allows the agent to learn from historical data.	<b>3)</b>	 It simplifies the computation of Q-values.	<b>4)</b>	 It enables the agent to update its policy online.	<b>ANS)</b>	B	<br><br>
20	Which technique is commonly used to address the exploration-exploitation dilemma in reinforcement learning?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Value iteration	<b>3)</b>	 Q-learning	<b>4)</b>	 Epsilon-greedy strategy	<b>ANS)</b>	D	<br><br>
21	What is the main challenge of using function approximation in reinforcement learning?	<br><b>1)</b>	 It requires large amounts of memory to store Q-values.	<b>2)</b>	 It leads to overfitting and poor generalization.	<b>3)</b>	 It slows down the convergence of value iteration algorithms.	<b>4)</b>	 It introduces approximation errors in value estimates.	<b>ANS)</b>	D	<br><br>
22	Which reinforcement learning algorithm is based on the idea of learning a value function directly from observations?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Deep Q-Networks (DQN)	<b>3)</b>	Policy Gradient methods	<b>4)</b>	 Monte Carlo methods	<b>ANS)</b>	A	<br><br>
23	In reinforcement learning, what does the term "exploitation" refer to?	<br><b>1)</b>	 Trying out new actions to gather more information	<b>2)</b>	 Selecting actions that are known to yield high rewards	<b>3)</b>	 Updating the value function using past experiences	<b>4)</b>	 Balancing the trade-off between exploration and exploitation	<b>ANS)</b>	B	<br><br>
24	Which technique is used to encourage exploration in Q-learning algorithms?	<br><b>1)</b>	 Random action selection	<b>2)</b>	Temporal Difference (T learning	<b>3)</b>	 Boltzmann exploration	<b>4)</b>	 Epsilon-greedy strategy	<b>ANS)</b>	D	<br><br>
25	What is the primary role of the actor in Actor-Critic methods?	<br><b>1)</b>	 To learn the value function by approximating the expected rewards	<b>2)</b>	 To evaluate the performance of the critic and provide feedback	<b>3)</b>	 To select actions based on the current policy	<b>4)</b>	 To update the policy parameters using gradient ascent	<b>ANS)</b>	C	<br><br>
26	Which technique is used to estimate the advantage function in Advantage Actor-Critic (A2 algorithms?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Monte Carlo simulation	<b>3)</b>	 Value iteration	<b>4)</b>	 Importance sampling	<b>ANS)</b>	A	<br><br>
27	What is the primary role of the value function in Actor-Critic methods?	<br><b>1)</b>	 To approximate the gradient of the policy function	<b>2)</b>	 To evaluate the performance of the actor and provide feedback	<b>3)</b>	 To estimate the expected future rewards under a given policy	<b>4)</b>	 To select actions based on their estimated advantages	<b>ANS)</b>	C	<br><br>
28	Which technique is used to update the policy parameters in policy gradient methods?	<br><b>1)</b>	 Gradient descent	<b>2)</b>	 Random search	<b>3)</b>	 Evolutionary algorithms	<b>4)</b>	 Tabular methods	<b>ANS)</b>	A	<br><br>
29	What is the main advantage of using natural policy gradients in reinforcement learning?	<br><b>1)</b>	 They improve the stability and convergence of policy updates.	<b>2)</b>	 They provide a more efficient way to explore the state-action space.	<b>3)</b>	 They enable the agent to handle continuous action spaces.	<b>4)</b>	 They reduce the computational complexity of value function approximation.	<b>ANS)</b>	A	<br><br>
30	Which reinforcement learning algorithm is based on the idea of learning a policy directly from state observations?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Policy Gradient methods	<b>3)</b>	 Deep Q-Networks (DQN)	<b>4)</b>	 Temporal Difference (T learning	<b>ANS)</b>	B	<br><br>
31	What is the main advantage of using eligibility traces in temporal difference (T learning?	<br><b>1)</b>	 They speed up convergence by incorporating past experiences.	<b>2)</b>	 They prevent the agent from getting stuck in local optima.	<b>3)</b>	 They provide a systematic way to explore the state-action space.	<b>4)</b>	 They enable the agent to handle continuous action spaces.	<b>ANS)</b>	A	<br><br>
32	Which technique is commonly used to address the exploration-exploitation dilemma in reinforcement learning?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Value iteration	<b>3)</b>	 Q-learning	<b>4)</b>	 Epsilon-greedy strategy	<b>ANS)</b>	D	<br><br>
33	What is the main advantage of using off-policy learning in reinforcement learning?	<br><b>1)</b>	 It guarantees convergence to the optimal policy.	<b>2)</b>	 It allows the agent to learn from historical data.	<b>3)</b>	 It simplifies the computation of Q-values.	<b>4)</b>	 It enables the agent to update its policy online.	<b>ANS)</b>	B	<br><br>
34	Which technique is commonly used to estimate the value function in policy gradient methods?	<br><b>1)</b>	 Monte Carlo simulation	<b>2)</b>	 Temporal Difference (T learning	<b>3)</b>	 Bellman equation	<b>4)</b>	 Importance sampling	<b>ANS)</b>	B	<br><br>
35	What is the primary role of the critic in Actor-Critic methods?	<br><b>1)</b>	 To learn the policy by maximizing the expected reward	<b>2)</b>	 To evaluate the value of state-action pairs	<b>3)</b>	 To explore the environment and gather experience	<b>4)</b>	 To approximate the gradient of the policy function	<b>ANS)</b>	B	<br><br>
36	Which technique is commonly used to reduce the variance of policy gradient estimates?	<br><b>1)</b>	 Baseline subtraction	<b>2)</b>	 Importance sampling	<b>3)</b>	 Temporal Difference (T learning	<b>4)</b>	 Experience replay	<b>ANS)</b>	A	<br><br>
37	In Actor-Critic methods, what is the role of the actor?	<br><b>1)</b>	 To learn the value function by approximating the expected rewards	<b>2)</b>	 To evaluate the performance of the critic and provide feedback	<b>3)</b>	 To select actions based on the current policy	<b>4)</b>	 To update the policy parameters using gradient ascent	<b>ANS)</b>	C	<br><br>
38	Which technique is commonly used to estimate the advantage function in Advantage Actor-Critic (A2 algorithms?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Monte Carlo simulation	<b>3)</b>	 Value iteration	<b>4)</b>	 Importance sampling	<b>ANS)</b>	A	<br><br>
39	What is the primary role of the value function in Actor-Critic methods?	<br><b>1)</b>	 To approximate the gradient of the policy function	<b>2)</b>	 To evaluate the performance of the actor and provide feedback	<b>3)</b>	 To estimate the expected future rewards under a given policy	<b>4)</b>	 To select actions based on their estimated advantages	<b>ANS)</b>	C	<br><br>
40	Which technique is used to update the policy parameters in policy gradient methods?	<br><b>1)</b>	 Gradient descent	<b>2)</b>	 Random search	<b>3)</b>	 Evolutionary algorithms	<b>4)</b>	 Tabular methods	<b>ANS)</b>	A	<br><br>
41	What is the main advantage of using natural policy gradients in reinforcement learning?	<br><b>1)</b>	 They improve the stability and convergence of policy updates.	<b>2)</b>	 They provide a more efficient way to explore the state-action space.	<b>3)</b>	 They enable the agent to handle continuous action spaces.	<b>4)</b>	 They reduce the computational complexity of value function approximation.	<b>ANS)</b>	A	<br><br>
42	Which reinforcement learning algorithm is based on the idea of learning a policy directly from state observations?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Policy Gradient methods	<b>3)</b>	 Deep Q-Networks (DQN)	<b>4)</b>	 Temporal Difference (T learning	<b>ANS)</b>	B	<br><br>
43	What is the main advantage of using eligibility traces in temporal difference (T learning?	<br><b>1)</b>	 They speed up convergence by incorporating past experiences.	<b>2)</b>	 They prevent the agent from getting stuck in local optima.	<b>3)</b>	 They provide a systematic way to explore the state-action space.	<b>4)</b>	 They enable the agent to handle continuous action spaces.	<b>ANS)</b>	A	<br><br>
44	Which technique is commonly used to address the exploration-exploitation dilemma in reinforcement learning?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Value iteration	<b>3)</b>	 Q-learning	<b>4)</b>	 Epsilon-greedy strategy	<b>ANS)</b>	D	<br><br>
45	What is the main advantage of using off-policy learning in reinforcement learning?	<br><b>1)</b>	 It guarantees convergence to the optimal policy.	<b>2)</b>	 It allows the agent to learn from historical data.	<b>3)</b>	 It simplifies the computation of Q-values.	<b>4)</b>	 It enables the agent to update its policy online.	<b>ANS)</b>	B	<br><br>
46	Which technique is commonly used to estimate the value function in policy gradient methods?	<br><b>1)</b>	 Monte Carlo simulation	<b>2)</b>	 Temporal Difference (T learning	<b>3)</b>	 Bellman equation	<b>4)</b>	 Importance sampling	<b>ANS)</b>	B	<br><br>
47	In reinforcement learning, what does the term "exploitation" refer to?	<br><b>1)</b>	 Trying out new actions to gather more information about the environment	<b>2)</b>	 Maximizing the cumulative reward by selecting actions that are currently believed to be the best	<b>3)</b>	 Balancing exploration and exploitation to achieve the optimal policy	<b>4)</b>	 Randomly selecting actions to learn about their consequences	<b>ANS)</b>	B	<br><br>
48	Which technique is commonly used to update the Q-values in Q-learning?	<br><b>1)</b>	 Gradient descent	<b>2)</b>	 Temporal Difference (T learning	<b>3)</b>	 Policy iteration	<b>4)</b>	 Boltzmann exploration	<b>ANS)</b>	B	<br><br>
49	What is the main advantage of using function approximation techniques in reinforcement learning?	<br><b>1)</b>	 It allows for more efficient computation of Q-values.	<b>2)</b>	 It reduces the need for exploration in the state-action space.	<b>3)</b>	 It enables learning in high-dimensional state spaces.	<b>4)</b>	 It guarantees convergence to the global optimum.	<b>ANS)</b>	C	<br><br>
50	Which algorithm is used to estimate the value function in the SARSA (State-Action-Reward-State-Action) algorithm?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Monte Carlo methods	<b>3)</b>	 Temporal Difference (T learning	<b>4)</b>	 Policy Gradient methods	<b>ANS)</b>	C	<br><br>
51	What is the primary objective of policy iteration algorithms in reinforcement learning?	<br><b>1)</b>	 To directly learn the optimal policy from state transitions.	<b>2)</b>	 To iteratively improve the policy and value function estimates.	<b>3)</b>	 To estimate the value function using dynamic programming.	<b>4)</b>	 To update the policy parameters using gradient ascent.	<b>ANS)</b>	B	<br><br>
52	Which algorithm combines both model-based and model-free reinforcement learning approaches?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Monte Carlo methods	<b>3)</b>	 Dyna-Q	<b>4)</b>	 SARSA (State-Action-Reward-State-Action)	<b>ANS)</b>	C	<br><br>
53	In reinforcement learning, what is the discount factor used for?	<br><b>1)</b>	 To penalize actions that lead to negative rewards	<b>2)</b>	 To weigh immediate rewards more than future rewards	<b>3)</b>	 To determine the rate of exploration in the environment	<b>4)</b>	 To ensure convergence of the value function estimates	<b>ANS)</b>	B	<br><br>
54	Which reinforcement learning algorithm is specifically designed for environments with continuous state and action spaces?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Deep Q-Networks (DQN)	<b>3)</b>	 Deterministic Policy Gradient (DPG)	<b>4)</b>	 Monte Carlo methods	<b>ANS)</b>	C	<br><br>
55	What is the main advantage of using off-policy learning in reinforcement learning?	<br><b>1)</b>	 It guarantees convergence to the optimal policy.	<b>2)</b>	 It allows the agent to learn from historical data.	<b>3)</b>	 It simplifies the computation of Q-values.	<b>4)</b>	 It enables the agent to update its policy online.	<b>ANS)</b>	B	<br><br>
56	Which technique is commonly used to address the exploration-exploitation dilemma in reinforcement learning?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Value iteration	<b>3)</b>	 Q-learning	<b>4)</b>	 Epsilon-greedy strategy	<b>ANS)</b>	D	<br><br>
57	What is the primary advantage of using value iteration algorithms in reinforcement learning?	<br><b>1)</b>	 They guarantee convergence to the optimal policy.	<b>2)</b>	 They provide unbiased estimates of the value function.	<b>3)</b>	 They enable the agent to handle continuous action spaces.	<b>4)</b>	 They allow for more efficient exploration of the state-action space.	<b>ANS)</b>	A	<br><br>
58	Which technique is commonly used to estimate the value function in policy gradient methods?	<br><b>1)</b>	 Monte Carlo simulation	<b>2)</b>	 Temporal Difference (T learning	<b>3)</b>	 Bellman equation	<b>4)</b>	 Importance sampling	<b>ANS)</b>	B	<br><br>
59	What is the primary role of the critic in Actor-Critic methods?	<br><b>1)</b>	 To learn the policy by maximizing the expected reward	<b>2)</b>	 To evaluate the value of state-action pairs	<b>3)</b>	 To explore the environment and gather experience	<b>4)</b>	 To approximate the gradient of the policy function	<b>ANS)</b>	B	<br><br>
60	Which technique is commonly used to reduce the variance of policy gradient estimates?	<br><b>1)</b>	 Baseline subtraction	<b>2)</b>	 Importance sampling	<b>3)</b>	 Temporal Difference (T learning	<b>4)</b>	 Experience replay	<b>ANS)</b>	A	<br><br>
61	What is the objective of off-policy learning in reinforcement learning?	<br><b>1)</b>	 To learn the optimal policy directly from expert demonstrations	<b>2)</b>	 To update the policy based on the agent's own experiences	<b>3)</b>	 To estimate the value function using samples generated by a different policy	<b>4)</b>	 To maximize the expected reward without considering future states	<b>ANS)</b>	C	<br><br>
62	What is the primary objective of the policy in reinforcement learning?	<br><b>1)</b>	 To maximize the expected cumulative reward	<b>2)</b>	 To minimize the exploration in the environment	<b>3)</b>	 To accurately predict the value function	<b>4)</b>	 To select actions randomly in the environment	<b>ANS)</b>	A	<br><br>
63	Which technique is commonly used to estimate the advantage function in Advantage Actor-Critic (A2 algorithms?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Monte Carlo simulation	<b>3)</b>	 Value iteration	<b>4)</b>	 Importance sampling	<b>ANS)</b>	A	<br><br>
64	What is the main advantage of using eligibility traces in reinforcement learning?	<br><b>1)</b>	 They speed up convergence by incorporating past experiences.	<b>2)</b>	 They prevent the agent from getting stuck in local optima.	<b>3)</b>	 They provide a systematic way to explore the state-action space.	<b>4)</b>	 They enable the agent to handle continuous action spaces.	<b>ANS)</b>	A	<br><br>
65	Which reinforcement learning algorithm is specifically designed for environments with continuous state and action spaces?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Deep Q-Networks (DQN)	<b>3)</b>	 Deterministic Policy Gradient (DPG)	<b>4)</b>	 Monte Carlo methods	<b>ANS)</b>	C	<br><br>
66	What is the main advantage of using off-policy learning in reinforcement learning?	<br><b>1)</b>	 It guarantees convergence to the optimal policy.	<b>2)</b>	 It allows the agent to learn from historical data.	<b>3)</b>	 It simplifies the computation of Q-values.	<b>4)</b>	 It enables the agent to update its policy online.	<b>ANS)</b>	B	<br><br>
67	Which technique is commonly used to address the exploration-exploitation dilemma in reinforcement learning?	<br><b>1)</b>	 Temporal Difference (T learning	<b>2)</b>	 Value iteration	<b>3)</b>	 Q-learning	<b>4)</b>	 Epsilon-greedy strategy	<b>ANS)</b>	D	<br><br>
68	What is the primary advantage of using value iteration algorithms in reinforcement learning?	<br><b>1)</b>	 They guarantee convergence to the optimal policy.	<b>2)</b>	 They provide unbiased estimates of the value function.	<b>3)</b>	 They enable the agent to handle continuous action spaces.	<b>4)</b>	 They allow for more efficient exploration of the state-action space.	<b>ANS)</b>	A	<br><br>
69	Which technique is commonly used to estimate the value function in policy gradient methods?	<br><b>1)</b>	 Monte Carlo simulation	<b>2)</b>	 Temporal Difference (T learning	<b>3)</b>	 Bellman equation	<b>4)</b>	 Importance sampling	<b>ANS)</b>	B	<br><br>
70	What is the primary role of the critic in Actor-Critic methods?	<br><b>1)</b>	 To learn the policy by maximizing the expected reward	<b>2)</b>	 To evaluate the value of state-action pairs	<b>3)</b>	 To explore the environment and gather experience	<b>4)</b>	 To approximate the gradient of the policy function	<b>ANS)</b>	B	<br><br>
71	Which technique is commonly used to reduce the variance of policy gradient estimates?	<br><b>1)</b>	 Baseline subtraction	<b>2)</b>	 Importance sampling	<b>3)</b>	 Temporal Difference (T learning	<b>4)</b>	 Experience replay	<b>ANS)</b>	A	<br><br>
72	What is the objective of off-policy learning in reinforcement learning?	<br><b>1)</b>	 To learn the optimal policy directly from expert demonstrations	<b>2)</b>	 To update the policy based on the agent's own experiences	<b>3)</b>	 To estimate the value function using samples generated by a different policy	<b>4)</b>	 To maximize the expected reward without considering future states	<b>ANS)</b>	C	<br><br>
73	Which algorithm is commonly used to approximate the value function in the Monte Carlo method?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 Temporal Difference (T learning	<b>3)</b>	 Bellman equation	<b>4)</b>	 First-visit Monte Carlo	<b>ANS)</b>	D	<br><br>
74	What is the primary objective of policy iteration algorithms in reinforcement learning?	<br><b>1)</b>	 To directly learn the optimal policy from state transitions.	<b>2)</b>	 To iteratively improve the policy and value function estimates.	<b>3)</b>	 To estimate the value function using dynamic programming.	<b>4)</b>	 To update the policy parameters using gradient ascent.	<b>ANS)</b>	B	<br><br>
75	Which reinforcement learning algorithm is specifically designed to learn from delayed rewards?	<br><b>1)</b>	 Q-learning	<b>2)</b>	 SARSA (State-Action-Reward-State-Action)	<b>3)</b>	 Deep Q-Networks (DQN)	<b>4)</b>	 Policy Gradient methods	<b>ANS)</b>	A	<br><br>
76	What is Q-learning?	<br><b>1)</b>	A type of Monte Carlo method	<b>2)</b>	B. An off-policy temporal-difference learning algorithm	<b>3)</b>	A form of policy gradient method	<b>4)</b>	A type of on-policy control algorithm	<b>ANS)</b>	B	<br><br>
77	In Q-learning, what does the Q-value represent	<br><b>1)</b>	Reward received for taking action a in state s	<b>2)</b>	Expected cumulative reward for taking action a in state s and following the optimal policy thereafter	<b>3)</b>	Value of taking action a in state s under the current policy	<b>4)</b>	Probability of taking action a in state s	<b>ANS)</b>	B	<br><br>
78	What is the update equation for Q-learning?	<br><b>1)</b>	Q(s, a) ← R(s, a) + γ * max_a' Q(s', a')	<b>2)</b>	Q(s, a) ← R(s, a) + γ * Q(s', a')	<b>3)</b>	Q(s, a) ← Q(s, a) + α * [R(s, a) + γ * max_a' Q(s', a') - Q(s, a)]	<b>4)</b>	Q(s, a) ← Q(s, a) + α * [R(s, a) + γ * Q(s', a') - Q(s, a)]	<b>ANS)</b>	C	<br><br>
79	What is n-step bootstrapping in reinforcement learning?	<br><b>1)</b>	It involves updating the Q-values based on the rewards received after n time steps	<b>2)</b>	It refers to a method for estimating state values using n-step returns	<b>3)</b>	It is a form of Monte Carlo method	<b>4)</b>	It updates the Q-values using the rewards received after every nth episode	<b>ANS)</b>	B	<br><br>
80	In n-step bootstrapping, what is the backup diagram for updating state values?	<br><b>1)</b>	A single-step backup	<b>2)</b>	A single-step backup	<b>3)</b>	A single-step backup	<b>4)</b>	A temporal-difference backup	<b>ANS)</b>	D	<br><br>
81	What is the advantage of using n-step bootstrapping over single-step updates?	<br><b>1)</b>	It requires less memory	<b>2)</b>	It reduces the variance of the update	<b>3)</b>	It converges faster	<b>4)</b>	It is simpler to implement	<b>ANS)</b>	B	<br><br>
82	How does the choice of n affect the performance of n-step bootstrapping?	<br><b>1)</b>	Higher n values lead to faster convergence	<b>2)</b>	Lower n values lead to lower variance	<b>3)</b>	Higher n values increase the computational complexity	<b>4)</b>	Lower n values increase the computational efficiency	<b>ANS)</b>	C	<br><br>
83	What is Dynamic Programming (DP) in the context of reinforcement learning?	<br><b>1)</b>	A method for solving decision-making problems by breaking them down into smaller subproblems and solving them recursively	<b>2)</b>	A technique for estimating state-action values using temporal differences	<b>3)</b>	A method for training deep neural networks in reinforcement learning	<b>4)</b>	A method for generating random policies for exploration	<b>ANS)</b>	A	<br><br>
84	What is the main idea behind DP's efficiency in reinforcement learning?	<br><b>1)</b>	It relies on large-scale parallel computing	<b>2)</b>	It avoids exploration of the state-action space	<b>3)</b>	It reuses solutions to subproblems to avoid redundant computations	<b>4)</b>	It relies on stochastic gradient descent for optimization	<b>ANS)</b>	C	<br><br>
85	Which of the following is true about DP's computational efficiency?	<br><b>1)</b>	It guarantees the global optimal solution without any computational cost	<b>2)</b>	It scales poorly with the size of the state and action spaces	<b>3)</b>	It is only applicable to episodic tasks	<b>4)</b>	It requires extensive exploration of the environment	<b>ANS)</b>	B	<br><br>
86	What is the Bellman equation used for in Dynamic Programming?	<br><b>1)</b>	It represents the value of each state-action pair	<b>2)</b>	It defines the optimal policy for a given MDP	<b>3)</b>	It describes the relationship between the value of a state and the values of its successor states	<b>4)</b>	It calculates the temporal difference error in reinforcement learning algorithms	<b>ANS)</b>	C	<br><br>
87	How does DP's computational complexity depend on the size of the problem?	<br><b>1)</b>	It grows linearly with the size of the problem	<b>2)</b>	It grows exponentially with the size of the problem	<b>3)</b>	It remains constant regardless of the size of the problem	<b>4)</b>	It depends on the specific structure of the problem, not just its size	<b>ANS)</b>	B	<br><br>
88	Which of the following is a characteristic of Dynamic Programming (DP) algorithms?	<br><b>1)</b>	They require exploration of the entire state-action space.	<b>2)</b>	They are only applicable to problems with a small number of states	<b>3)</b>	They guarantee finding the optimal solution without approximation.	<b>4)</b>	They rely on function approximation to estimate state values.	<b>ANS)</b>	C	<br><br>
89	In Dynamic Programming, what is the term "bootstrapping" referring to?	<br><b>1)</b>	he process of updating value estimates based on estimates from previous iterations.	<b>2)</b>		<b>3)</b>	The method of randomly initializing value functions before training	<b>4)</b>	The process of exploring the environment to gather information	<b>ANS)</b>	A	<br><br>
90	Which of the following statements is true regarding the computational efficiency of Dynamic Programming?	<br><b>1)</b>	It requires re-evaluation of the entire state space in each iteration.	<b>2)</b>	It is highly efficient for problems with continuous state and action spaces.	<b>3)</b>	It is less efficient when the problem has a high degree of stochasticity.	<b>4)</b>	It scales well with the size of the state and action spaces.	<b>ANS)</b>	C	<br><br>
91	What is one of the key limitations of Dynamic Programming in reinforcement learning?	<br><b>1)</b>	It cannot handle problems with continuous state spaces.	<b>2)</b>	It requires prior knowledge of the environment's dynamics.	<b>3)</b>	It cannot handle non-Markovian decision processes.	<b>4)</b>	It suffers from the curse of dimensionality.	<b>ANS)</b>	D	<br><br>
92	How does the efficiency of Dynamic Programming algorithms change as the size of the state space increases?	<br><b>1)</b>	It improves due to more opportunities for exploitation	<b>2)</b>	. It deteriorates due to increased computational complexity.	<b>3)</b>	It remains constant regardless of the size of the state space.	<b>4)</b>	It depends on the specific structure of the problem	<b>ANS)</b>	B	<br><br>
93	Which of the following statements best describes the relationship between Dynamic Programming and reinforcement learning?	<br><b>1)</b>	Dynamic Programming is a subset of reinforcement learning algorithms.	<b>2)</b>	Reinforcement learning is a subset of Dynamic Programming algorithms.	<b>3)</b>	Dynamic Programming and reinforcement learning are entirely separate approaches.	<b>4)</b>	Dynamic Programming is used to solve reinforcement learning problems.	<b>ANS)</b>	D	<br><br>
94	What are Monte Carlo methods in reinforcement learning?	<br><b>1)</b>	A type of dynamic programming approach	<b>2)</b>	A family of model-free estimation method	<b>3)</b>	A form of policy gradient algorithm	<b>4)</b>	A type of heuristic search algorithm	<b>ANS)</b>	B	<br><br>
95	What is the primary advantage of Monte Carlo methods?	<br><b>1)</b>	They are computationally efficient for large state spaces	<b>2)</b>	They provide exact estimates of state values	<b>3)</b>	They do not require knowledge of the environment's dynamics	<b>4)</b>	They guarantee convergence to the optimal policy	<b>ANS)</b>	D	<br><br>
96	How do Monte Carlo methods estimate state values?	<br><b>1)</b>	By computing the average of all possible state transitions	<b>2)</b>	By sampling trajectories and averaging returns	<b>3)</b>	By using temporal differences between successive states	<b>4)</b>	By applying value iteration to the Bellman equation	<b>ANS)</b>	B	<br><br>
97	Which of the following is a disadvantage of Monte Carlo methods?	<br><b>1)</b>	They suffer from high variance	<b>2)</b>	They require a model of the environment	<b>3)</b>	They are computationally complex for large state spaces	<b>4)</b>	They cannot handle continuous state and action spaces	<b>ANS)</b>	A	<br><br>
98	In Monte Carlo methods, what does the term "Monte Carlo" refer to?	<br><b>1)</b>	A specific reinforcement learning algorithm	<b>2)</b>	The city in Monaco where the method was first developed	<b>3)</b>	The use of random sampling to estimate numerical results	<b>4)</b>	A famous mathematician who pioneered the method	<b>ANS)</b>	C	<br><br>
99	How do Monte Carlo methods handle the exploration-exploitation trade-off?	<br><b>1)</b>	hey use epsilon-greedy exploration	<b>2)</b>	They rely on dynamic programming principles	<b>3)</b>	They explore all state-action pairs uniformly	<b>4)</b>	They balance exploration and exploitation through random sampling	<b>ANS)</b>	D	<br><br>
100	What type of problem is Monte Carlo methods particularly suitable for?	<br><b>1)</b>	Problems with continuous state spaces	<b>2)</b>	Problems with known transition dynamics	<b>3)</b>	Problems with a small number of states	<b>4)</b>	Problems with episodic environments	<b>ANS)</b>	D	<br><br>
101	Which of the following is a characteristic of Gradient Bandit Algorithms	<br><b>1)</b>	They are based on greedy selection of actions.	<b>2)</b>	They update action preferences based on rewards received.	<b>3)</b>	They prioritize exploration over exploitation.	<b>4)</b>	They utilize Q-values for action selection.	<b>ANS)</b>	B	<br><br>
102	In a finite Markov Decision Process (MDP), which component describes the set of all possible states?	<br><b>1)</b>	Transition probability matrix	<b>2)</b>	Policy function	<b>3)</b>	State space	<b>4)</b>	Reward function	<b>ANS)</b>	C	<br><br>
103	In the context of reinforcement learning, an optimal policy is the one that:	<br><b>1)</b>	Maximizes the immediate reward only.	<b>2)</b>	Maximizes the cumulative reward over time	<b>3)</b>	Is randomly selected.	<b>4)</b>	Minimizes the state space.	<b>ANS)</b>	B	<br><br>
104	What role does the "baseline" play in Gradient Bandit Algorithms?	<br><b>1)</b>	It determines the maximum possible reward.	<b>2)</b>	It influences the exploration rate.	<b>3)</b>	It helps in estimating the expected reward.	<b>4)</b>	It stabilizes learning by providing a reference point.	<b>ANS)</b>	D	<br><br>
105	Which of the following best describes the transition probability matrix in a finite Markov Decision Process?	<br><b>1)</b>	It specifies the reward received in each state.	<b>2)</b>	It indicates the probability of transitioning from one state to another based on chosen actions.	<b>3)</b>	It outlines the optimal policy for each state.	<b>4)</b>	It represents the discount factor for future rewards.	<b>ANS)</b>	B	<br><br>
106	What is the primary objective of finding an optimal value function in reinforcement learning?	<br><b>1)</b>	To minimize the computational complexity of the learning algorithm.	<b>2)</b>	To maximize the variance of rewards obtained.	<b>3)</b>	To estimate the expected cumulative reward under a given policy.	<b>4)</b>	To prioritize exploration over exploitation.	<b>ANS)</b>	C	<br><br>
107	How does the step-size parameter affect the learning process in Gradient Bandit Algorithms?	<br><b>1)</b>	It controls the rate of exploration.	<b>2)</b>	It determines the number of episodes in the training process.	<b>3)</b>	It regulates the magnitude of updates to action preferences.	<b>4)</b>	It influences the selection of the next action.	<b>ANS)</b>	C	<br><br>
108	In a finite Markov Decision Process, what does the discount factor signify?	<br><b>1)</b>	The rate at which the environment changes.	<b>2)</b>	The probability of transitioning between states.	<b>3)</b>	The importance of future rewards relative to immediate rewards.	<b>4)</b>	The maximum possible reward attainable in the environment.	<b>ANS)</b>	C	<br><br>
109	What is the primary difference between on-policy and off-policy methods in reinforcement learning?	<br><b>1)</b>	On-policy methods optimize the policy currently being followed, while off-policy methods optimize a different policy.	<b>2)</b>	On-policy methods require more computational resources compared to off-policy methods.	<b>3)</b>	On-policy methods do not consider exploration, while off-policy methods prioritize exploration over exploitation.	<b>4)</b>	On-policy methods rely solely on Q-values for action selection, while off-policy methods utilize gradient descent algorithms.	<b>ANS)</b>	A	<br><br>
110	Which of the following is a drawback of Gradient Bandit Algorithms?	<br><b>1)</b>	They are computationally expensive.	<b>2)</b>	They are sensitive to initial parameter values.	<b>3)</b>	They require a large amount of training data.	<b>4)</b>	They are biased towards exploration.	<b>ANS)</b>	B	<br><br>
111	What is the Bellman Equation used for in reinforcement learning?	<br><b>1)</b>	To calculate the total number of states in the environment.	<b>2)</b>	To estimate the expected cumulative reward for each action in a given state.	<b>3)</b>	To iteratively update the value function based on the principle of optimality.	<b>4)</b>	To determine the optimal exploration-exploitation trade-off.	<b>ANS)</b>	D	<br><br>
112	In the context of Markov Decision Processes (MDPs), what does the term "Markov" refer to?	<br><b>1)</b>	The property that future states depend only on the current state and not on the past states.	<b>2)</b>	The mathematical function used to calculate rewards.	<b>3)</b>	The process of selecting actions based on a stochastic policy.	<b>4)</b>	The technique used to initialize Q-values in Q-learning algorithms.	<b>ANS)</b>	A	<br><br>
113	13. Which of the following statements is true regarding the exploration-exploitation dilemma in reinforcement learning?	<br><b>1)</b>	Exploration is always prioritized over exploitation to discover the optimal policy.	<b>2)</b>	Exploitation is always prioritized over exploration to maximize short-term rewards.	<b>3)</b>	Balancing exploration and exploitation is crucial for finding the optimal policy.	<b>4)</b>	The exploration-exploitation dilemma is irrelevant in reinforcement learning.	<b>ANS)</b>	C  	<br><br>
114	In reinforcement learning, what is the role of the value function?	<br><b>1)</b>	To estimate the expected reward for taking a particular action in a given state.	<b>2)</b>	To determine the optimal policy for each state.	<b>3)</b>	To keep track of the number of times each action has been taken in each state	<b>4)</b>	To regulate the learning rate during training.	<b>ANS)</b>	A	<br><br>
115	What is the main purpose of the reward function in a Markov Decision Process?	<br><b>1)</b>	To define the set of all possible states in the environment.	<b>2)</b>	To specify the probability of transitioning between states.	<b>3)</b>	To provide feedback to the agent about the desirability of states and actions.	<b>4)</b>	To determine the discount factor for future rewards.	<b>ANS)</b>	C	<br><br>
116	Which of the following statements best describes the softmax action selection strategy in Gradient Bandit Algorithms?	<br><b>1)</b>	It selects actions based on their estimated action values, with a probability proportional to their values.	<b>2)</b>	It always chooses the action with the highest estimated value.	<b>3)</b>	It randomly selects actions with equal probability.	<b>4)</b>	It prioritizes exploration by selecting actions with the lowest estimated values.	<b>ANS)</b>	A	<br><br>
117	What is the relationship between the value function and the Q-function in reinforcement learning?	<br><b>1)</b>	The value function is the expected cumulative reward under a given policy, while the Q-function estimates the value of taking a specific action in a given state	<b>2)</b>	The value function and the Q-function are interchangeable terms used to describe the expected reward for each action in a given state.	<b>3)</b>	The value function represents the maximum possible reward attainable in the environment, while the Q-function determines the optimal policy.	<b>4)</b>	The value function is derived from the Q-function through a logarithmic transformation.	<b>ANS)</b>	A	<br><br>
118	What does the term "finite" signify in Finite Markov Decision Processes?	<br><b>1)</b>	It indicates that the environment has a finite number of states and actions.	<b>2)</b>	It implies that the transition probabilities between states are bounded within a finite range.	<b>3)</b>	It refers to the finite duration of each episode in the learning process.	<b>4)</b>	It suggests that the rewards received are limited to finite values.	<b>ANS)</b>	A	<br><br>
119	In the context of Gradient Bandit Algorithms, what is the significance of the learning rate parameter?	<br><b>1)</b>	It controls the rate at which the exploration-exploitation trade-off is adjusted.	<b>2)</b>	It determines the number of episodes in the training process.	<b>3)</b>	It influences the step size of parameter updates during learning.	<b>4)</b>	It regulates the balance between exploration and exploitation.	<b>ANS)</b>	C	<br><br>
120	Which of the following is a key characteristic of an optimal policy in reinforcement learning?	<br><b>1)</b>	It always selects the action with the highest immediate reward.	<b>2)</b>	It maximizes the expected cumulative reward over time.	<b>3)</b>	It relies solely on past experiences without considering future rewards.	<b>4)</b>	It ignores the exploration-exploitation trade-off.	<b>ANS)</b>	B	<br><br>
121	Which of the following best describes the concept of planning in the context of artificial intelligence?	<br><b>1)</b>	Planning involves making decisions without considering future consequences.	<b>2)</b>		<b>3)</b>	Planning is the process of selecting actions to achieve a goal based on available knowledge and resources.	<b>4)</b>	Planning focuses solely on short-term objectives without considering long-term goals.	<b>ANS)</b>	B	<br><br>
122	What is the primary goal of search algorithms in artificial intelligence?	<br><b>1)</b>	To minimize the time required for computation	<b>2)</b>	To find the optimal solution in the shortest amount of time	<b>3)</b>	To efficiently explore a problem space to find a solution	<b>4)</b>	To avoid exploring the problem space entirely	<b>ANS)</b>	C	<br><br>
123	In the context of planning, what does the term "state space" refer to?	<br><b>1)</b>	The physical space in which actions are executed	<b>2)</b>	The physical space in which actions are executed	<b>3)</b>	The set of all possible states that the environment can be in	<b>4)</b>	The sequence of actions required to achieve a goal	<b>ANS)</b>	C	<br><br>
124	What is the role of a heuristic function in informed search algorithms?	<br><b>1)</b>	To guide the search towards the optimal solution by providing an estimate of the cost from the current state to the goal	<b>2)</b>	To generate random actions to explore the problem space	<b>3)</b>	To systematically explore all possible states in the state space	<b>4)</b>	To systematically explore all possible states in the state space	<b>ANS)</b>	A	<br><br>
125	Which optimization technique is commonly used to solve planning problems with discrete state and action spaces?	<br><b>1)</b>	Gradient descent	<b>2)</b>	Simulated annealing	<b>3)</b>	Dynamic programming	<b>4)</b>	Genetic algorithms	<b>ANS)</b>	C	<br><br>
126	In the context of game playing, what is the main objective of adversarial search algorithms?	<br><b>1)</b>	To find the optimal strategy for a single player	<b>2)</b>	To search for the best move without considering the opponent's actions	<b>3)</b>	To anticipate the opponent's moves and choose the best response	<b>4)</b>	To explore all possible game states regardless of their relevance	<b>ANS)</b>	C	<br><br>
127	What is the fundamental difference between model-based and model-free reinforcement learning approaches?	<br><b>1)</b>	Model-based methods rely on pre-defined rules, while model-free methods learn directly from experience.	<b>2)</b>	Model-based methods require explicit knowledge of the environment dynamics, while model-free methods learn directly from trial and error.	<b>3)</b>	Model-based methods use heuristics to guide the search for the optimal solution, while model-free methods use gradient descent algorithms.	<b>4)</b>	Model-based methods are more computationally expensive than model-free methods.	<b>ANS)</b>	B	<br><br>
128	What is the key advantage of Monte Carlo Tree Search (MCTS) over traditional search algorithms in game playing?	<br><b>1)</b>	It guarantees finding the optimal solution in all cases.	<b>2)</b>	It uses a heuristic function to guide the search towards the best move.	<b>3)</b>	It samples from the state space and uses the results to guide the search.	<b>4)</b>	It explores all possible game states exhaustively.	<b>ANS)</b>	C	<br><br>
129	What is a key assumption of Markov Decision Processes (MDPs) in reinforcement learning?	<br><b>1)</b>	Actions taken in one state do not affect future states.	<b>2)</b>	Actions taken in one state have a deterministic effect on future states.	<b>3)</b>	Rewards are always deterministic and fixed for each action.	<b>4)</b>	The environment dynamics are fully observable.	<b>ANS)</b>	A	<br><br>
130	Which search algorithm guarantees finding the shortest path in a graph with non-negative edge weights?	<br><b>1)</b>	Breadth-first search (BFS)	<b>2)</b>	Depth-first search (DFS)	<b>3)</b>	Dijkstra's algorithm	<b>4)</b>	A* search algorithm	<b>ANS)</b>	C	<br><br>
131	In a Constraint Satisfaction Problem, what does a constraint define?	<br><b>1)</b>	A constraint defines a rule that limits the possible assignments of values to variables.	<b>2)</b>	A constraint defines the optimal solution to the problem.	<b>3)</b>	A constraint defines the total cost of a solution.	<b>4)</b>	A constraint defines the number of variables in the problem.	<b>ANS)</b>	A	<br><br>
132	What is the primary principle behind simulated annealing?	<br><b>1)</b>	Simulated annealing mimics the process of metal cooling, gradually reducing the system's temperature to find a low-energy state.	<b>2)</b>	Simulated annealing uses a fixed learning rate to update model parameters.	<b>3)</b>	Simulated annealing applies random perturbations to the solution and accepts changes that lead to improvements.	<b>4)</b>	Simulated annealing uses a heuristic function to guide the search towards the goal state.	<b>ANS)</b>	A	<br><br>
133	What is the primary goal of the value iteration algorithm in reinforcement learning?	<br><b>1)</b>	To learn the optimal policy directly from experience.	<b>2)</b>	To estimate the expected cumulative reward for each action in each state.	<b>3)</b>	To iteratively update the value function until convergence to the optimal values.	<b>4)</b>	To apply reinforcement learning in environments with continuous state and action spaces.	<b>ANS)</b>	C	<br><br>
134	What is the main inspiration behind genetic algorithms?	<br><b>1)</b>	The process of natural selection and evolution	<b>2)</b>	The principles of calculus and differential equations	<b>3)</b>	The concept of gradient descent	<b>4)</b>	The principles of probability theory	<b>ANS)</b>	A	<br><br>
135	What is the heuristic function used for in the A* search algorithm?	<br><b>1)</b>	To estimate the cost of reaching the goal from the current state	<b>2)</b>	To randomly select actions during the search process	<b>3)</b>	To explore all possible states in the state space	<b>4)</b>	o determine the optimal policy for each state	<b>ANS)</b>	A	<br><br>
136	What is the primary advantage of dynamic programming in solving planning problems?	<br><b>1)</b>	It guarantees finding the optimal solution in all cases.	<b>2)</b>	It is computationally efficient and can handle large state spaces.	<b>3)</b>	It requires minimal memory resources compared to other approaches.	<b>4)</b>	It is robust to changes in the environment dynamics.	<b>ANS)</b>	B	<br><br>
137	In probabilistic graphical models, what does a Bayesian network represent?	<br><b>1)</b>	A directed acyclic graph representing the dependencies between random variables	<b>2)</b>	An undirected graph representing the conditional independence between random variables	<b>3)</b>	A graph representing the joint probability distribution of random variables	<b>4)</b>	A graph representing the marginal probabilities of random variables	<b>ANS)</b>	S	<br><br>
138	What property defines a Markov chain?	<br><b>1)</b>	The memoryless property, where the future state depends only on the current state and not on the past states.	<b>2)</b>	The ability to represent non-deterministic transitions between states.	<b>3)</b>	The presence of a cyclic graph structure.	<b>4)</b>	The requirement for all states to have the same transition probabilities.	<b>ANS)</b>	A	<br><br>
139	What is the key principle behind Bayesian inference?	<br><b>1)</b>	Updating beliefs based on observed evidence and prior knowledge using Bayes' theorem.	<b>2)</b>	Minimizing the error between predicted and actual outcomes.	<b>3)</b>	Fitting a model to the data using maximum likelihood estimation.	<b>4)</b>	Applying statistical tests to assess the significance of results.	<b>ANS)</b>	A	<br><br>
140	Which optimization technique is commonly used in stochastic optimization to find the minimum of a function?	<br><b>1)</b>	Gradient descent	<b>2)</b>	Simulated annealing	<b>3)</b>	Genetic algorithms	<b>4)</b>	Ant colony optimization	<b>ANS)</b>	A	<br><br>
141	What approach is commonly used to handle uncertainty in planning problems?	<br><b>1)</b>	Deterministic planning	<b>2)</b>	Probabilistic planning	<b>3)</b>	Heuristic search	<b>4)</b>	Evolutionary algorithms	<b>ANS)</b>	B	<br><br>
142	What is the primary goal of utility theory in decision making?	<br><b>1)</b>	Maximizing the expected value of outcomes	<b>2)</b>	Minimizing the risk associated with decisions	<b>3)</b>	Balancing trade-offs between conflicting objectives	<b>4)</b>	Satisficing, or finding satisfactory solutions rather than optimal ones	<b>ANS)</b>	A	<br><br>
143	What is the purpose of a knowledge representation language in artificial intelligence?	<br><b>1)</b>	To define the syntax and semantics of natural language	<b>2)</b>	To represent and manipulate knowledge in a structured format	<b>3)</b>	To generate random samples from a probability distribution	<b>4)</b>	To generate random samples from a probability distribution	<b>ANS)</b>	B	<br><br>
144	What is the primary objective of the Dyna architecture in reinforcement learning?	<br><b>1)</b>	To integrate planning with learning from real experience	<b>2)</b>	To integrate planning with learning from real experience	<b>3)</b>	To minimize the computational complexity of reinforcement learning algorithms	<b>4)</b>	To model the environment dynamics using neural networks	<b>ANS)</b>	D	<br><br>
145	How does Dyna differ from traditional model-based reinforcement learning algorithms?	<br><b>1)</b>	Dyna focuses solely on learning from real experience without planning.	<b>2)</b>	Dyna does not require a model of the environment dynamics.	<b>3)</b>	Dyna combines both learning from real experience and planning using a simulated model.	<b>4)</b>	Dyna relies on heuristics to guide the exploration process.	<b>ANS)</b>	D	<br><br>
146	In Dyna-Q, what does the action-value function QQQ represent?	<br><b>1)</b>	The expected cumulative reward for taking a specific action in a given state	<b>2)</b>	The probability distribution of future rewards for each action	<b>3)</b>	The transition probabilities between states	<b>4)</b>	The discounted cumulative reward from the initial state to the goal state	<b>ANS)</b>	A	<br><br>
147	How does Dyna address the exploration-exploitation trade-off?	<br><b>1)</b>	By always selecting the action with the highest expected reward	<b>2)</b>	By using a random policy for action selection	<b>3)</b>	By balancing real experience with planning based on past experiences	<b>4)</b>	By minimizing the exploration rate over time	<b>ANS)</b>	C	<br><br>
148	What role does the planning component play in the Dyna architecture?	<br><b>1)</b>	It generates random actions to explore the environment.	<b>2)</b>	It simulates the environment model to update action values based on hypothetical experiences.	<b>3)</b>	It memorizes past experiences for future reference.	<b>4)</b>	It adjusts the learning rate during the training process.	<b>ANS)</b>	B	<br><br>
149	In Dyna-Q+, how are real experiences utilized for learning?	<br><b>1)</b>	Real experiences are ignored in favor of planning.	<b>2)</b>	Real experiences are used to update action values directly.	<b>3)</b>	Real experiences are used to update the environment model.	<b>4)</b>	Real experiences are stored in memory for later analysis.	<b>ANS)</b>	B	<br><br>
150	What approach does Dyna use for planning based on the environment model?	<br><b>1)</b>	Symbolic logic	<b>2)</b>	Sample-based planning	<b>3)</b>	Reinforcement learning	<b>4)</b>	Genetic algorithms	<b>ANS)</b>	B	<br><br>
151	Which reinforcement learning algorithm does Dyna-Q integrate with planning?	<br><b>1)</b>	SARSA	<b>2)</b>	Q-learning	<b>3)</b>	Policy gradient methods	<b>4)</b>	Temporal Difference (TD) learning	<b>ANS)</b>	B	<br><br>
152	What distinguishes Dyna from purely model-free reinforcement learning algorithms?	<br><b>1)</b>	Dyna does not require learning from real experiences.	<b>2)</b>	Dyna combines model-free learning with planning based on a learned model of the environment.	<b>3)</b>	Dyna relies solely on planning without considering real experiences.	<b>4)</b>	Dyna uses a heuristic function to guide action selection.	<b>ANS)</b>	B	<br><br>
153	What information does the environment model in Dyna typically include?	<br><b>1)</b>	Only the transition probabilities between states and actions	<b>2)</b>	The rewards associated with each state-action pair	<b>3)</b>	Both the transition probabilities and the rewards	<b>4)</b>	A list of all possible states in the environment	<b>ANS)</b>	C	<br><br>
154	How does the efficiency of planning in Dyna affect its performance?	<br><b>1)</b>	More planning iterations lead to better performance, regardless of computational cost.	<b>2)</b>	Planning should be balanced with real experience to avoid excessive computational overhead.	<b>3)</b>	Planning is not essential for performance; real experience alone is sufficient.	<b>4)</b>	Planning should prioritize exploration over exploitation to achieve better performance.	<b>ANS)</b>	B	<br><br>
155	How does the learning rate parameter impact the performance of Dyna?	<br><b>1)</b>	A higher learning rate accelerates learning from real experiences but may destabilize planning.	<b>2)</b>	A lower learning rate improves the stability of planning but slows down learning from real experiences.	<b>3)</b>	The learning rate has no impact on Dyna's performance.	<b>4)</b>	The learning rate determines the balance between exploration and exploitation.	<b>ANS)</b>	A	<br><br>
156	Which exploration strategy is commonly used in Dyna-Q to balance exploration and exploitation?	<br><b>1)</b>	Epsilon-greedy	<b>2)</b>	Boltzmann exploration	<b>3)</b>	CB1 (Upper Confidence Bound)	<b>4)</b>	Thompson sampling	<b>ANS)</b>	A	<br><br>
157	How does the size of the environment model affect the memory requirements of Dyna?	<br><b>1)</b>	A larger environment model requires less memory due to compression techniques.	<b>2)</b>	A smaller environment model reduces memory requirements but may sacrifice planning accuracy.	<b>3)</b>	The size of the environment model has no impact on Dyna's memory requirements.	<b>4)</b>	Larger environment models increase memory requirements, potentially limiting scalability.	<b>ANS)</b>	D	<br><br>
158	How is temporal-difference learning utilized in Dyna?	<br><b>1)</b>	Temporal-difference learning is the primary method for planning in Dyna.	<b>2)</b>	Temporal-difference learning is used to update action values based on real experiences.	<b>3)</b>	Temporal-difference learning is not applicable in Dyna.	<b>4)</b>	Temporal-difference learning is used to evaluate the accuracy of the environment model.	<b>ANS)</b>	B	<br><br>
159	What is the Stanislavski Method primarily focused on?	<br><b>1)</b>	Physical movement	<b>2)</b>	Emotional realism	<b>3)</b>	Vocal projection	<b>4)</b>	Stage blocking	<b>ANS)</b>	B	<br><br>
160	Which acting technique emphasizes the use of imagination and sense memory?	<br><b>1)</b>	Method acting	<b>2)</b>	Classical acting	<b>3)</b>	Meisner technique	<b>4)</b>	Viewpoints technique	<b>ANS)</b>	A	<br><br>
161	Who is often credited as the father of modern acting theory?	<br><b>1)</b>	Konstantin Stanislavski	<b>2)</b>	Jerzy Grotowski	<b>3)</b>	Anton Chekhov	<b>4)</b>	Bertolt Brecht	<b>ANS)</b>	A	<br><br>
162	Which of the following is NOT a key element of the Meisner technique?	<br><b>1)</b>	Repetition	<b>2)</b>	Emotional memory	<b>3)</b>	Listening	<b>4)</b>	Impulse	<b>ANS)</b>	B	<br><br>
163	In acting, what does the term "breaking character" refer to?	<br><b>1)</b>	Forgetting lines during a performance	<b>2)</b>	The actor portraying a different emotion than intended	<b>3)</b>	Losing concentration and focus on stage	<b>4)</b>	An intentional shift in performance style	<b>ANS)</b>	A	<br><br>
164	What is the purpose of vocal warm-up exercises for actors?	<br><b>1)</b>	o improve projection	<b>2)</b>	To develop accents and dialects	<b>3)</b>	To increase breath control	<b>4)</b>	All of the above	<b>ANS)</b>	D	<br><br>
165	Which type of theater is known for breaking the fourth wall and directly addressing the audience?	<br><b>1)</b>	Absurdist theater	<b>2)</b>	Epic theater	<b>3)</b>	Naturalistic theater	<b>4)</b>	Interactive theater	<b>ANS)</b>	D	<br><br>
166	What is the term for the area where actors wait when they are not on stage?	<br><b>1)</b>	Green room	<b>2)</b>	Wings	<b>3)</b>	Backstage	<b>4)</b>	Dressing room	<b>ANS)</b>	A	<br><br>
167	What is the purpose of a character arc in a script?	<br><b>1)</b>	To provide backstory for the character	<b>2)</b>	To outline the character's journey and development	<b>3)</b>	To establish the setting of the story	<b>4)</b>	To introduce conflict and tension	<b>ANS)</b>	B	<br><br>
168	What does the term "blocking" refer to in theater?	<br><b>1)</b>	The process of writing stage directions	<b>2)</b>	he arrangement of actors' movements on stage	<b>3)</b>	The rehearsal process for a play	<b>4)</b>	The design and construction of sets	<b>ANS)</b>	B	<br><br>
169	Who is considered the founder of the "Viewpoints" technique, a method of movement improvisation for actors?	<br><b>1)</b>	Anne Bogart	<b>2)</b>	Tadashi Suzuki	<b>3)</b>	Jerzy Grotowski	<b>4)</b>	Mary Overlie	<b>ANS)</b>	D	<br><br>
170	What does the term "typecasting" mean in the context of acting?	<br><b>1)</b>	Casting actors based on their physical appearance or personality traits	<b>2)</b>	Casting actors for roles that challenge their usual type	<b>3)</b>	Casting actors in roles without considering their suitability	<b>4)</b>	Casting actors in roles based solely on their popularity	<b>ANS)</b>	A	<br><br>
171	Which of the following is NOT a method for actors to memorize their lines?	<br><b>1)</b>	Asking the director for a teleprompter Answer: d) Asking the director for a teleprompte	<b>2)</b>	epeating lines out loud	<b>3)</b>	Writing lines down repeatedly	<b>4)</b>	Using mnemonic devices	<b>ANS)</b>	A	<br><br>
172	What is the purpose of a script analysis for actors?	<br><b>1)</b>	To understand the historical context of the play	<b>2)</b>	To interpret the director's vision for the production	<b>3)</b>	To analyze the themes, characters, and relationships in the script	<b>4)</b>	To learn about the technical aspects of theater production	<b>ANS)</b>	C	<br><br>
173	In the "Chekhov Technique," what does the term "psychological gesture" refer to?	<br><b>1)</b>	Physical movements that express a character's emotions	<b>2)</b>	The use of props to symbolize a character's inner thoughts	<b>3)</b>	Vocal techniques to convey a character's psychological state	<b>4)</b>	Collaborative exercises between actors to build trust and rapport	<b>ANS)</b>	A	<br><br>
174	What is the primary goal of improvisational theater?	<br><b>1)</b>	To follow a scripted storyline	<b>2)</b>	To create spontaneous performances without prior preparation	<b>3)</b>	To incorporate audience suggestions into the performance	<b>4)</b>	To adhere strictly to traditional acting techniques	<b>ANS)</b>	B	<br><br>
175	Which learning technique involves sampling possible trajectories in a system to understand its behavior?	<br><b>1)</b>	Trajectory Sampling	<b>2)</b>	Real-time Dynamic Programming	<b>3)</b>	Planning at Decision Time	<b>4)</b>	Heuristic Search	<b>ANS)</b>	A	<br><br>
176	What is the primary focus of Real-time Dynamic Programming?	<br><b>1)</b>	Finding optimal solutions in real-time	<b>2)</b>	Planning ahead without considering real-time constraints	<b>3)</b>	Incorporating dynamic elements into static plans	<b>4)</b>	Minimizing computational complexity	<b>ANS)</b>	A	<br><br>
177	Planning at Decision Time is most commonly associated with which field?	<br><b>1)</b>	Robotics	<b>2)</b>	Natural Language Processing	<b>3)</b>	Computer Graphics	<b>4)</b>	Machine Learning	<b>ANS)</b>	D	<br><br>
178	Which algorithmic approach aims to efficiently explore a search space by prioritizing promising options?	<br><b>1)</b>	Heuristic Search	<b>2)</b>	Rollout Algorithms	<b>3)</b>	Monte Carlo Tree Search	<b>4)</b>	Trajectory Sampling	<b>ANS)</b>	A	<br><br>
179	Rollout Algorithms are often used in which type of problem-solving?	<br><b>1)</b>	Deterministic planning	<b>2)</b>	Stochastic planning	<b>3)</b>	Symbolic reasoning	<b>4)</b>	Constraint satisfaction	<b>ANS)</b>	B	<br><br>
180	Monte Carlo Tree Search (MCTS) is particularly effective in which kind of environments?	<br><b>1)</b>	Fully observable environments	<b>2)</b>	Deterministic environments	<b>3)</b>	Partially observable and stochastic environments	<b>4)</b>	Adversarial environments	<b>ANS)</b>	C	<br><br>
181	What is the main advantage of Monte Carlo Tree Search compared to traditional tree search algorithms?	<br><b>1)</b>	It guarantees finding the optimal solution.	<b>2)</b>	It requires less memory.	<b>3)</b>	It handles uncertainty and randomness well.	<b>4)</b>	It has a faster runtime.	<b>ANS)</b>	C	<br><br>
182	In the context of Monte Carlo Tree Search, what does the term "rollout" refer to?	<br><b>1)</b>	Simulating the rest of the game from a given state to determine its value	<b>2)</b>	Expanding the search tree based on heuristics	<b>3)</b>	Selecting the most promising node to explore next	<b>4)</b>	Propagating back the results of simulations to update node values	<b>ANS)</b>	A	<br><br>
183	Which of the following is NOT a phase in the standard Monte Carlo Tree Search algorithm?	<br><b>1)</b>	Which of the following is NOT a phase in the standard Monte Carlo Tree Search algorithm?	<b>2)</b>	Expansion	<b>3)</b>	Contraction	<b>4)</b>	Simulation	<b>ANS)</b>	C	<br><br>
184	How does Monte Carlo Tree Search differ from traditional tree search algorithms like Minimax?	<br><b>1)</b>	Monte Carlo Tree Search does not use any form of backtracking.	<b>2)</b>	Monte Carlo Tree Search does not use any form of backtracking.	<b>3)</b>	Monte Carlo Tree Search always explores the entire search space.	<b>4)</b>	Monte Carlo Tree Search is only applicable to games with perfect information.	<b>ANS)</b>	B	<br><br>
185	What is the primary goal of trajectory sampling in machine learning?	<br><b>1)</b>	Finding the optimal solution in a given search space	<b>2)</b>	Sampling possible trajectories to understand system behavior	<b>3)</b>	Constructing decision trees based on training data	<b>4)</b>	Performing feature selection in high-dimensional data	<b>ANS)</b>	B	<br><br>
186	Real-time Dynamic Programming is best suited for problems that require:	<br><b>1)</b>	Finding optimal solutions offline	<b>2)</b>	Dealing with uncertainty and partial observability	<b>3)</b>	Adapting to changing environments in real-time	<b>4)</b>	Solving problems with finite and discrete state spaces	<b>ANS)</b>	C	<br><br>
187	Planning at Decision Time is most commonly used in the field of:	<br><b>1)</b>	Reinforcement Learning	<b>2)</b>	Natural Language Processing	<b>3)</b>	Image Recognition	<b>4)</b>	Genetic Algorithms	<b>ANS)</b>	A	<br><br>
188	In heuristic search algorithms, the heuristic function is used to:	<br><b>1)</b>	Generate random samples from the search space	<b>2)</b>	Evaluate the quality of a solution without exhaustively searching the entire space	<b>3)</b>	Update the parameters of the learning model during training	<b>4)</b>	Estimate the probability distribution of future states	<b>ANS)</b>	B	<br><br>
189	Rollout algorithms are particularly effective in problems that involve:	<br><b>1)</b>	Fully deterministic environments	<b>2)</b>	Adversarial decision-making	<b>3)</b>	Static and unchanging state spaces	<b>4)</b>	Continuous optimization problems	<b>ANS)</b>	B	<br><br>
190	Monte Carlo Tree Search (MCTS) has been successfully applied in:	<br><b>1)</b>	Solving linear programming problems	<b>2)</b>	Approximating solutions to integrals	<b>3)</b>	Playing board games like Go and Chess	<b>4)</b>	Predicting time series data	<b>ANS)</b>	D	<br><br>
191	What does the term "rollout" typically refer to in the context of Monte Carlo Tree Search?	<br><b>1)</b>	Performing a series of random actions to explore possible trajectories	<b>2)</b>	Expanding the search tree by adding new nodes	<b>3)</b>	Backpropagating the results of simulations to update node values	<b>4)</b>	Selecting the most promising node to explore next	<b>ANS)</b>	A	<br><br>
192	Which phase of Monte Carlo Tree Search involves selecting nodes based on certain criteria such as the Upper Confidence Bound (UCB)?	<br><b>1)</b>	Selection	<b>2)</b>	Expansion	<b>3)</b>	Simulation	<b>4)</b>	Backpropagation	<b>ANS)</b>	A	<br><br>
193	What distinguishes Monte Carlo Tree Search from traditional tree search algorithms like Minimax?	<br><b>1)</b>	Monte Carlo Tree Search does not use any form of backtracking.	<b>2)</b>	Monte Carlo Tree Search guarantees finding the optimal solution.	<b>3)</b>	Monte Carlo Tree Search focuses on exploration through random simulations.	<b>4)</b>	Monte Carlo Tree Search requires complete information about the game state.	<b>ANS)</b>	C	<br><br>
194	In the context of Monte Carlo Tree Search, what does the term "rollout policy" refer to?	<br><b>1)</b>	The set of rules governing the expansion of the search tree	<b>2)</b>	The strategy for selecting nodes to explore during the selection phase	<b>3)</b>	The algorithm used to simulate the rest of the game from a given state	<b>4)</b>	The procedure for updating node values based on simulation results	<b>ANS)</b>	C	<br><br>
195	Trajectory Sampling is particularly useful in understanding:	<br><b>1)</b>	Theoretical concepts in mathematics	<b>2)</b>	Dynamic systems and their behavior over time	<b>3)</b>	Historical events and their impact on society	<b>4)</b>	Linguistic patterns in natural languages	<b>ANS)</b>	B	<br><br>
196	Planning at Decision Time is often utilized in:	<br><b>1)</b>	Chess-playing algorithms	<b>2)</b>	Sentiment analysis in social media	<b>3)</b>	Automated theorem proving	<b>4)</b>	Self-driving car navigation systems	<b>ANS)</b>	D	<br><br>
197	Heuristic Search algorithms typically use heuristic functions to:	<br><b>1)</b>	Generate random samples from the search space	<b>2)</b>	Guide the search towards promising solutions	<b>3)</b>	Guarantee finding the global optimum	<b>4)</b>	Reduce the complexity of the search space	<b>ANS)</b>	B	<br><br>
198	Rollout algorithms are particularly suitable for problems with:	<br><b>1)</b>	Static and deterministic environments	<b>2)</b>	Unpredictable and stochastic outcomes	<b>3)</b>	Short-term memory constraints 	<b>4)</b>	A limited number of possible actions	<b>ANS)</b>	B	<br><br>
199	Monte Carlo Tree Search (MCTS) has demonstrated success in:	<br><b>1)</b>	Solving linear algebraic equations	<b>2)</b>	Approximating integrals in calculus	<b>3)</b>	Playing complex strategy games like AlphaGo	<b>4)</b>	Optimizing supply chain logistics	<b>ANS)</b>	C	<br><br>
200	In Monte Carlo Tree Search, the "rollout phase" involves:	<br><b>1)</b>	Expanding the search tree with additional nodes	<b>2)</b>	Simulating future states from a given game state	<b>3)</b>	Selecting the most promising node for exploration	<b>4)</b>	Backpropagating simulation results to update node values	<b>ANS)</b>	B	<br><br>

</blockquote>

</body>
</html>

